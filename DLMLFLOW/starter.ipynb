{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quickstart: {Compare runs, choose a model and reploy it to a REST API}\n",
    "\n",
    "#### Run hyperaparameter sweep on a training script\n",
    "#### compare results of runs in the MLflow UI\n",
    "#### Choose best run and register it as a model\n",
    "#### deploy model to a REST API\n",
    "#### Build a container image suitable for deployment to a cloud platform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "from hyperopt import STATUS_OK, Trials, fmin, hp, tpe\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from mlflow.models import infer_signature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/mlflow/mlflow/master/tests/datasets/winequality-white.csv\",sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data into training, validation and test sets\n",
    "\n",
    "train,test=train_test_split(data,test_size=.25,random_state=42)\n",
    "\n",
    "#training dataset\n",
    "train_x = train.drop(['quality'],axis=1).values\n",
    "train_y = train[['quality']].values.ravel() #.ravel() allows us to get a single dimension array, otherwise .values() produces a 2D array\n",
    "\n",
    "#test dataset\n",
    "test_x = test.drop(['quality'],axis=1).values\n",
    "test_y = test[['quality']].values.ravel()\n",
    "\n",
    "#split the train data into train validation AGAIN, to check the performance of the model\n",
    "train_x,valid_x,train_y,valid_y=train_test_split(train_x,train_y,test_size=.2,random_state=42)\n",
    "\n",
    "signature = infer_signature(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ANN Model\n",
    "\n",
    "def train_model(params,epochs,train_x,train_y,valid_x,valid_y,test_x,test_y):\n",
    "    ## Define model architecture\n",
    "    mean = np.mean(train_x,axis=0) #the mean of each feature, for normalization\n",
    "    var = np.var(train_x,axis=0)\n",
    "\n",
    "    model=keras.Sequential([\n",
    "        keras.Input([train_x.shape[1]]), #11 input features in the input layer\n",
    "        keras.layers.Normalization(mean=mean,variance=var), #apply normalization to input features\n",
    "        keras.layers.Dense(64,activation='relu'), #hidden layers\n",
    "        keras.layers.Dense(1) #output layer\n",
    "    ])\n",
    "\n",
    "    ## compile the model\n",
    "    model.compile(optimizer=keras.optimizers.SGD(\n",
    "        learning_rate=params['lr'],momentum=params['momentum'] #hyper parameters\n",
    "        ),\n",
    "        loss='mean_squared_error', #tracking\n",
    "        metrics=[keras.metrics.RootMeanSquaredError()]\n",
    "    )\n",
    "\n",
    "    ## Train the ANN model with lr and momentum params with MLFLOW tracking\n",
    "    with mlflow.start_run(nested=True):\n",
    "        model.fit(train_x,train_y,validation_data=(valid_x,valid_y), epochs=epochs, batch_size=64)\n",
    "\n",
    "        ## Evaluate the model\n",
    "        eval_result = model.evaluate(valid_x,valid_y,batch_size=64)\n",
    "\n",
    "        eval_rmse=eval_result[1]\n",
    "\n",
    "        ## log the parameters and results\n",
    "        mlflow.log_params(params)\n",
    "        mlflow.log_metric(\"eval_rmse\",eval_rmse)\n",
    "\n",
    "        ## log the model\n",
    "        mlflow.tensorflow.log_model(model, 'model',signature=signature)\n",
    "\n",
    "        return {'loss':eval_rmse,\"status\":STATUS_OK,\"model\":model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(params):\n",
    "    # MLflow will track params and results for each run\n",
    "    result = train_model(\n",
    "        params,\n",
    "        epochs=3,\n",
    "        train_x=train_x,\n",
    "        train_y=train_y,\n",
    "        valid_x=valid_x,\n",
    "        valid_y=valid_y,\n",
    "        test_x=test_x,\n",
    "        test_y=test_y\n",
    "    )\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "space={\n",
    "    \"lr\":hp.loguniform(\"lr\",np.log(1e-5),np.log(1e-1)), #establishes the learning rate range\n",
    "    'momentum':hp.uniform('momentum',0.0,1.0) #momentum param range\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025/01/09 21:01:28 INFO mlflow.tracking.fluent: Experiment with name '/wine-quality' does not exist. Creating a new experiment.\n",
      "TPE is being used as the default algorithm.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3                                            \n",
      "\n",
      "  0%|          | 0/4 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 21:01:28.735724: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m34s\u001b[0m 756ms/step - loss: 42.2024 - root_mean_squared_error: 6.4963\n",
      "\u001b[1m11/46\u001b[0m \u001b[32m笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 15.1715 - root_mean_squared_error: 3.7257   \n",
      "\u001b[1m33/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 8.0035 - root_mean_squared_error: 2.6331 \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - loss: 6.4048 - root_mean_squared_error: 2.3321 - val_loss: 0.6430 - val_root_mean_squared_error: 0.8019\n",
      "\n",
      "Epoch 2/3                                            \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 0.5452 - root_mean_squared_error: 0.7384\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6086 - root_mean_squared_error: 0.7800 - val_loss: 0.5340 - val_root_mean_squared_error: 0.7308\n",
      "\n",
      "Epoch 3/3                                            \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m7s\u001b[0m 169ms/step - loss: 0.8692 - root_mean_squared_error: 0.9323\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.6209 - root_mean_squared_error: 0.7876 - val_loss: 0.5701 - val_root_mean_squared_error: 0.7551\n",
      "\n",
      "\u001b[1m 1/12\u001b[0m \u001b[32m笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.5183 - root_mean_squared_error: 0.7200\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.5369 - root_mean_squared_error: 0.7323 \n",
      "\n",
      "Epoch 1/3                                                                     \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m18s\u001b[0m 417ms/step - loss: 31.3174 - root_mean_squared_error: 5.5962\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 7.9848 - root_mean_squared_error: 2.6771 - val_loss: 1.4484 - val_root_mean_squared_error: 1.2035\n",
      "\n",
      "Epoch 2/3                                                                     \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.4419 - root_mean_squared_error: 1.2008\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2098 - root_mean_squared_error: 1.0996 - val_loss: 1.0292 - val_root_mean_squared_error: 1.0145\n",
      "\n",
      "Epoch 3/3                                                                     \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - loss: 1.0765 - root_mean_squared_error: 1.0375\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.9059 - root_mean_squared_error: 0.9516 - val_loss: 0.7998 - val_root_mean_squared_error: 0.8943\n",
      "\n",
      "\u001b[1m 1/12\u001b[0m \u001b[32m笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 0.6989 - root_mean_squared_error: 0.8360\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7933 - root_mean_squared_error: 0.8901 \n",
      "\n",
      "Epoch 1/3                                                                     \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m19s\u001b[0m 436ms/step - loss: 37.9412 - root_mean_squared_error: 6.1596\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - loss: 8.5324 - root_mean_squared_error: 2.7375 - val_loss: 1.3087 - val_root_mean_squared_error: 1.1440\n",
      "\n",
      "Epoch 2/3                                                                     \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6333 - root_mean_squared_error: 1.2780\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.2368 - root_mean_squared_error: 1.1101 - val_loss: 0.9089 - val_root_mean_squared_error: 0.9534\n",
      "\n",
      "Epoch 3/3                                                                     \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - loss: 0.7685 - root_mean_squared_error: 0.8767\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.8006 - root_mean_squared_error: 0.8947 - val_loss: 0.7311 - val_root_mean_squared_error: 0.8550\n",
      "\n",
      "\u001b[1m 1/12\u001b[0m \u001b[32m笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 0.7188 - root_mean_squared_error: 0.8478\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 0.7467 - root_mean_squared_error: 0.8639 \n",
      "\n",
      "Epoch 1/3                                                                     \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m18s\u001b[0m 403ms/step - loss: 33.1552 - root_mean_squared_error: 5.7581\n",
      "\u001b[1m23/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 26.7993 - root_mean_squared_error: 5.1608   \n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - loss: 21.2068 - root_mean_squared_error: 4.5537 - val_loss: 3.3985 - val_root_mean_squared_error: 1.8435\n",
      "\n",
      "Epoch 2/3                                                                     \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m14s\u001b[0m 326ms/step - loss: 3.1343 - root_mean_squared_error: 1.7704\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 2.6633 - root_mean_squared_error: 1.6309 - val_loss: 2.0435 - val_root_mean_squared_error: 1.4295\n",
      "\n",
      "Epoch 3/3                                                                     \n",
      "\n",
      "\u001b[1m 1/46\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m2s\u001b[0m 48ms/step - loss: 1.3850 - root_mean_squared_error: 1.1769\n",
      "\u001b[1m46/46\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7403 - root_mean_squared_error: 1.3183 - val_loss: 1.7547 - val_root_mean_squared_error: 1.3247\n",
      "\n",
      "\u001b[1m 1/12\u001b[0m \u001b[32m笏―u001b[0m\u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏―u001b[0m \u001b[1m0s\u001b[0m 13ms/step - loss: 1.4545 - root_mean_squared_error: 1.2060\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - loss: 1.7218 - root_mean_squared_error: 1.3111 \n",
      "\n",
      "100%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 4/4 [00:30<00:00,  7.60s/trial, best loss: 0.755068838596344]\n",
      "Best parameters: {'lr': np.float64(0.09963378607961039), 'momentum': np.float64(0.2752137379738513)}\n",
      "Best eval rmse: 0.755068838596344\n",
      "沛 View run receptive-lark-600 at: http://127.0.0.1:5000/#/experiments/748682491156821742/runs/73b5b660147f49c6a597a1801b70b7e9\n",
      "洫ｪ View experiment at: http://127.0.0.1:5000/#/experiments/748682491156821742\n"
     ]
    }
   ],
   "source": [
    "mlflow.set_experiment(\"/wine-quality\")\n",
    "with mlflow.start_run():\n",
    "    #conduct the hyperparameter search using Hyperopt\n",
    "    trials = Trials()\n",
    "    best=fmin(\n",
    "        fn=objective,\n",
    "        space=space,\n",
    "        max_evals=4,\n",
    "        trials=trials\n",
    "    )\n",
    "    \n",
    "    #fecth the details of the best run\n",
    "    best_run=sorted(trials.results,key=lambda x: x['loss'])[0]\n",
    "\n",
    "    #log the best paramters, loss and model\n",
    "    mlflow.log_params(best)\n",
    "    mlflow.log_metric(key='eval_rmse', value= best_run['loss'])\n",
    "    mlflow.tensorflow.log_model(best_run['model'], 'model', signature=signature)\n",
    "\n",
    "    print(f'Best parameters: {best}')\n",
    "    print(f'Best eval rmse: {best_run['loss']}')\n",
    "\n",
    "    mlflow.set_tracking_uri('http://127.0.0.1:5000')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Model and Inferencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading artifacts: 100%|笆遺毎笆遺毎笆遺毎笆遺毎笆遺毎| 7/7 [00:00<00:00, 1701.84it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 1/39\u001b[0m \u001b[37m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m \u001b[1m1s\u001b[0m 34ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m39/39\u001b[0m \u001b[32m笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤笏≫煤\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[6.272072 ],\n",
       "       [6.7174463],\n",
       "       [6.5126276],\n",
       "       ...,\n",
       "       [5.9886003],\n",
       "       [6.999872 ],\n",
       "       [5.755769 ]], dtype=float32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mlflow.models import validate_serving_input\n",
    "\n",
    "model_uri = 'runs:/71fcdaa1c43b4ca9942c97464f26a2c0/model'\n",
    "\n",
    "# The logged model does not contain an input_example.\n",
    "# Manually generate a serving payload to verify your model prior to deployment.\n",
    "from mlflow.models import convert_input_example_to_serving_input\n",
    "\n",
    "# Define INPUT_EXAMPLE via assignment with your own input example to the model\n",
    "# A valid input example is a data instance suitable for pyfunc prediction\n",
    "serving_payload = convert_input_example_to_serving_input(test_x)\n",
    "\n",
    "# Validate the serving payload works on the model\n",
    "validate_serving_input(model_uri, serving_payload)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
